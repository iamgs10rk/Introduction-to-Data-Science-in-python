{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk9Kp5J2f2K1oWiW+utiof",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamgs10rk/Introduction-to-Data-Science-in-python/blob/master/NN_All_Coding_Problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise Problems 02\n",
        "```\n",
        "def calculate_2d_mean_median_variance_columnwise(x):\n",
        " # given a two dimensional list of numbers.\n",
        " # Calculate mean, median and variance of each column\n",
        " # x: two dimensional list of numbers\n",
        " # return: two dimensional list of 3 rows by number of columns in x\n",
        " # The first row of the return list contains mean values,\n",
        " # 2nd row contains medians and third contains variances\n",
        " # Notes:\n",
        " # IMPORTANT:\n",
        " # Use only python. No other packages such as numpy, math, …\n",
        " # Assume that all the rows of x have the same number of elements\n",
        " ```\n",
        " ```\n",
        "def calculate_2d_ mean_median_variance_rowwise(x):\n",
        " # given a two dimensional list of numbers.\n",
        " # Calculate mean, median and variance of each row\n",
        " # x: two dimensional list of numbers\n",
        " # return: two dimensional list.\n",
        " # the number of rows in the return list should be the same as\n",
        " # number of rows in x and return list should have 3 columns\n",
        " # Notes:\n",
        " # Use only python. No other packages such as numpy, math, …\n",
        " # Assume that the number of elements in each row are NOT THE SAME\n",
        " ```\n",
        " ```\n",
        "def column_wise_normalization(x):\n",
        "# Write a function to normalize a numpy array, x, column-wise,\n",
        "# by performing # the following calculation for each element:\n",
        "# (element value - mean of its column) / standard deviation of its column.\n",
        "```\n",
        "```\n",
        "def mean_absolute_error(actual,desired):\n",
        "# This function calculates the mean absolute error\n",
        "# between two numpy arrays.\n",
        "```\n",
        "```\n",
        "def confusion_matrix(actual, desired):\n",
        "# This function computes the confusion matrix for a multi-class\n",
        "# classification problem using actual and desired values.\n",
        "# actual is a numpy array [input_dimensions,nof_samples]\n",
        "# desired is the same shape as actual\n",
        "```\n",
        "```\n",
        "def covariance_matrix(X):\n",
        "# This function calculates the covariance matrix\n",
        "# for a given numpy array.\n",
        "# Covariance Matrix = ((X-X_mean)^T* (X-X_mean))\n",
        "```\n",
        "[Link to Exercise Problems 02 PDF](https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/CSE-5368%20Exercise%20problems%2002.pdf)\n"
      ],
      "metadata": {
        "id": "8dveViYaqNP-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cvtZdz7-pmZ8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_2d_mean_median_variance_columnwise(x):\n",
        "    means = np.mean(x, axis=0)\n",
        "    medians = np.median(x, axis=0)\n",
        "    variances = np.var(x, axis=0)\n",
        "    return [means, medians, variances]\n",
        "\n",
        "def calculate_2d_mean_median_variance_rowwise(x):\n",
        "    means = np.mean(x, axis=1)\n",
        "    medians = np.median(x, axis=1)\n",
        "    variances = np.var(x, axis=1)\n",
        "    return np.array([means, medians, variances]).T\n",
        "\n",
        "def column_wise_normalization(x):\n",
        "    means = np.mean(x, axis=0)\n",
        "    std_devs = np.std(x, axis=0)\n",
        "    normalized_x = (x - means) / std_devs\n",
        "    return normalized_x\n",
        "\n",
        "def mean_absolute_error(actual, desired):\n",
        "    return np.mean(np.abs(actual - desired))\n",
        "\n",
        "def confusion_matrix(actual, desired):\n",
        "    num_classes = len(np.unique(actual))\n",
        "    confusion = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            confusion[i, j] = np.sum((actual == i) & (desired == j))\n",
        "    return confusion\n",
        "\n",
        "def covariance_matrix(X):\n",
        "    return np.cov(X)\n",
        "\n",
        "# Sample input\n",
        "x = np.array([[1, 2, 3],\n",
        "              [4, 5, 6],\n",
        "              [7, 8, 9]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input Array:\")\n",
        "print(x)\n",
        "\n",
        "print(\"\\nCalculate 2D Mean, Median, Variance Column-wise:\")\n",
        "result_columnwise = calculate_2d_mean_median_variance_columnwise(x)\n",
        "print(\"Means:\", result_columnwise[0])\n",
        "print(\"Medians:\", result_columnwise[1])\n",
        "print(\"Variances:\", result_columnwise[2])\n",
        "\n",
        "print(\"\\nCalculate 2D Mean, Median, Variance Row-wise:\")\n",
        "result_rowwise = calculate_2d_mean_median_variance_rowwise(x)\n",
        "print(result_rowwise)\n",
        "\n",
        "print(\"\\nColumn-wise Normalization:\")\n",
        "normalized_x = column_wise_normalization(x)\n",
        "print(normalized_x)\n",
        "\n",
        "print(\"\\nMean Absolute Error:\")\n",
        "actual = np.array([1, 2, 3])\n",
        "desired = np.array([2, 3, 4])\n",
        "mae = mean_absolute_error(actual, desired)\n",
        "print(mae)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "actual = np.array([0, 1, 1, 2, 2, 2])\n",
        "desired = np.array([0, 1, 2, 1, 2, 0])\n",
        "conf_matrix = confusion_matrix(actual, desired)\n",
        "print(conf_matrix)\n",
        "\n",
        "print(\"\\nCovariance Matrix:\")\n",
        "cov_matrix = covariance_matrix(x)\n",
        "print(cov_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd72EbJP2slj",
        "outputId": "e0ae240c-54cf-4720-fe9f-7464d75ac5d9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Array:\n",
            "[[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "\n",
            "Calculate 2D Mean, Median, Variance Column-wise:\n",
            "Means: [4. 5. 6.]\n",
            "Medians: [4. 5. 6.]\n",
            "Variances: [6. 6. 6.]\n",
            "\n",
            "Calculate 2D Mean, Median, Variance Row-wise:\n",
            "[[2.         2.         0.66666667]\n",
            " [5.         5.         0.66666667]\n",
            " [8.         8.         0.66666667]]\n",
            "\n",
            "Column-wise Normalization:\n",
            "[[-1.22474487 -1.22474487 -1.22474487]\n",
            " [ 0.          0.          0.        ]\n",
            " [ 1.22474487  1.22474487  1.22474487]]\n",
            "\n",
            "Mean Absolute Error:\n",
            "1.0\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1 0 0]\n",
            " [0 1 1]\n",
            " [1 1 1]]\n",
            "\n",
            "Covariance Matrix:\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise Problems 07\n",
        "\n",
        "Complete the code for the following function\n",
        "\n",
        "import numpy as np\n",
        "```\n",
        "def calculate_percent_error(self, YA, YT):\n",
        "    \"\"\"\n",
        "    Given a batch of input, actual outputs, and desired outputs, this function\n",
        "    calculates percent error. For each sample, if the actual output vector is not\n",
        "    exactly the same as the desired output, it is considered one error.\n",
        "    Percent error is 100*(number_of_errors/ number_of_samples)\n",
        "    \n",
        "    :param YA: Array of actual outputs [number_of_nodes, n_samples]. Assume that\n",
        "               each element of YA is either 0 or 1 (Result of hard-limit activation function).\n",
        "    :param YT: Array of desired (target) outputs [number_of_nodes, n_samples].\n",
        "               Assume that all the values of array YT are zero except one of them which is equal to 1.\n",
        "    :return: percent_error\n",
        "    \"\"\"\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/CSE-5368%20Exercise%20problems%2007.pdf"
      ],
      "metadata": {
        "id": "EWZkkrfSrzPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_percent_error(YA, YT):\n",
        "\n",
        "    assert YA.shape == YT.shape\n",
        "    error_count = np.sum(YA!=YT)\n",
        "    return (error_count/YA.shape[1])*100"
      ],
      "metadata": {
        "id": "68gbw1JJrpIO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example input data\n",
        "YA = np.array([[1, 0, 1], [0, 1, 1]])  # Actual outputs\n",
        "YT = np.array([[1, 0, 1], [1, 1, 0]])  # Desired outputs\n",
        "\n",
        "# Calculate percent error\n",
        "error_percentage = calculate_percent_error(YA, YT)\n",
        "print(\"Percent Error:\", error_percentage)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhGlaOQH2xUL",
        "outputId": "24e6754c-2237-4e27-b952-697cd987eef6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percent Error: 66.66666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the following code. The code should compute the output, loss, and gradients,\n",
        "and perform a single weight update of a neural network with 784 inputs, 100 nodes with\n",
        "sigmoid activation in the first layer, and 10 output nodes with linear activation.\n",
        "Assume you have a batch of inputs called X that has dimensions [16, 784],\n",
        "and a batch of targets called y that has dimensions [16, 1]\n",
        "\n",
        "```\n",
        "def my_loss(y, y_hat):\n",
        "    return tf.reduce_mean(\n",
        "           tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "           labels=y,logits=y_hat))\n",
        "# Create random weights and biases\n",
        "W_1 = tf.Variable(np.random.randn(                 ), trainable=True)\n",
        "b_1 = tf.Variable(np.random.randn(                 ), trainable=True)\n",
        "W_2 = tf.Variable(np.random.randn(                ), trainable=True)\n",
        "b_2 = tf.Variable(np.random.randn(                 ), trainable=True)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    # Calculate output\n",
        "    output=\n",
        "    # Calculate loss\n",
        "    loss\n",
        "    # Calculate gradients\n",
        "     \n",
        "    dW_1, db_1 =\n",
        "     \n",
        "    dW_2, db_2 =\n",
        "# Calculate new values of weights and biases\n",
        "```\n",
        "\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/CSE-5368%20Exercise%20problems%2007.pdf"
      ],
      "metadata": {
        "id": "txesOF7Jsa5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the following code. The code should compute the output, loss, and gradients,\n",
        "# and perform a single weight update of a neural network with 784 inputs, 100 nodes with\n",
        "# sigmoid activation in the first layer, and 10 output nodes with linear activation.\n",
        "# Assume you have a batch of inputs called X that has dimensions [16, 784],\n",
        "# and a batch of targets called y that has dimensions [16, 1]\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Create random weights and biases\n",
        "W_1 = tf.Variable(np.random.randn(784, 100), trainable=True)\n",
        "b_1 = tf.Variable(np.random.randn(100), trainable=True)\n",
        "W_2 = tf.Variable(np.random.randn(100, 10), trainable=True)\n",
        "b_2 = tf.Variable(np.random.randn(10), trainable=True)\n",
        "\n",
        "X = np.random.randn(16, 784)\n",
        "y = np.random.randn(16, 1)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    # Calculate output\n",
        "    output = tf.sigmoid(tf.matmul(X, W_1) + b_1)\n",
        "    output = tf.matmul(output, W_2) + b_2\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = tf.reduce_mean(\n",
        "               tf.nn.softmax(labels=tf.cast(y, dtype=tf.int32), logits=output))\n",
        "\n",
        "    # Calculate gradients\n",
        "    dW_1, db_1 = tape.gradient(loss, [W_1, b_1])\n",
        "    dW_2, db_2 = tape.gradient(loss, [W_2, b_2])\n",
        "\n",
        "# Calculate new values of weights and biases\n",
        "# Define the learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Update weights and biases using gradient descent\n",
        "W_1.assign_sub(learning_rate * dW_1)\n",
        "b_1.assign_sub(learning_rate * db_1)\n",
        "W_2.assign_sub(learning_rate * dW_2)\n",
        "b_2.assign_sub(learning_rate * db_2)\n"
      ],
      "metadata": {
        "id": "AQ14WRUatOjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the following function. Assume this function will be called by the main program shown below.\n",
        "```\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "def get_biases(model, layer_number=None, layer_name=None):\n",
        "“\"\"This function returns the biases for a layer.\n",
        ":param model: keras model\n",
        ":param layer_number: Layer number starting from layer 0\n",
        ":param layer_name: Layer name (if both layer_number and layer_name are\n",
        "specified, layer number takes precedence).\n",
        ":return: biases for the given layer (If the given layer does not have\n",
        "bias then None should be returned)\"\"\"\n",
        "\n",
        "my_model = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
        "for k in range(23):\n",
        " print(get_biases(model=my_model,layer_number=k))\n",
        " print(get_biases(model=my_model, layer_name=\"fc1\"))\n",
        " ```\n",
        " https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/CSE-5368%20Exercise%20problems%2007.pdf"
      ],
      "metadata": {
        "id": "zK5DD6pWtlsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "def get_biases(model, layer_number=None, layer_name=None):\n",
        "\n",
        "    if layer_name is not None:\n",
        "        layer = model.get_layer(layer_name)\n",
        "    elif layer_number is not None:\n",
        "        layer = model.layers[layer_number]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    return layer.bias.numpy() if layer.bias is not None else None\n",
        "\n",
        "my_model = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
        "for k in range(23):\n",
        "    print(get_biases(model=my_model, layer_number=k))\n",
        "print(get_biases(model=my_model, layer_name=\"fc1\"))\n"
      ],
      "metadata": {
        "id": "sJbAK9TQtx-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a single neuron with linear activation : 𝒐𝒖𝒕𝒑𝒖𝒕 ൌ 𝑾𝒙 ൅ 𝒃\n",
        "Write a Python function, using Tensorflow to adjust the weights and biases ( one\n",
        "step) and return the gradients of loss with respect to 𝑾 and gradients of loss with\n",
        "respect to 𝒃\n",
        "Notes:\n",
        "Assume loss is the defined as the MSE\n",
        "You must use Tensorflow without using Keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "```\n",
        "def calculate_loss(x,y,w,b):\n",
        "# x: input batch (input_dimensions, batch_size)\n",
        "# y: desired output (output_dimensions, batch_size)\n",
        "# w: weight matrix (input_dimensions, output_dimensions)\n",
        "# b: bias (1, output_dimensions)\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/CSE-5368%20Exercise%20problems%2007.pdf"
      ],
      "metadata": {
        "id": "uoUrAyKguZDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def calculate_loss(x, y, w, b):\n",
        "\n",
        "    # Forward pass\n",
        "    output = tf.matmul(tf.transpose(w), x) + b\n",
        "    loss = tf.reduce_mean(tf.square(output - y))  # Mean Squared Error (MSE)\n",
        "\n",
        "    # Compute gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch([w, b])\n",
        "        loss_value = tf.reduce_mean(tf.square(tf.matmul(tf.transpose(w), x) + b - y))\n",
        "    gradients = tape.gradient(loss_value, [w, b])\n",
        "\n",
        "    return loss, gradients[0], gradients[1]\n"
      ],
      "metadata": {
        "id": "xjIO4T7Utzr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the actual outputs and the desired output indexes of a neural network,\n",
        "calculate and return the confusion matrix. y_hat is a matrix presenting the\n",
        "actual output. Each row of this matrix is the actual output of the neural network for an input sample.\n",
        "\n",
        "The number of rows in this matrix is equal to the number of samples in\n",
        "the input batch. Y is the desired output. Each number in in the y is the index of the desired class for the corresponding input sample.\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "def calculate_confusion_matrix(y_hat, y):\n",
        "\"\"\"\n",
        "Given the actual outputs and the desired output index of a neural\n",
        "network, this function calculates the confusion matrix.\n",
        ":param y_hat: Array of actual outputs [num_of samples,num_of classes]\n",
        ":param y: Array of desired (target) outputs [num_of samples]. This array includes the indexes of the desired (true) class.\n",
        ":return confusion_matrix[number_of_classes,number_of_classes].\n",
        "Confusion matrix should be shown as the number of times that an input of class n is classified as class m.\"\"\"\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/CSE-5368%20Exercise%20problems%2007.pdf"
      ],
      "metadata": {
        "id": "FFfEjzfjuvaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSE-5368 Neural Networks Fall 2022 Quiz 05\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calculate_confusion_matrix(y_hat, y):\n",
        "\n",
        "    num_classes = y.shape[0]\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
        "    for i in range(y_hat.shape[0]):\n",
        "      confusion_matrix[i][np.argmax(y_hat[i])]  += 1\n",
        "    return confusion_matrix\n",
        "\n",
        "# Example input data\n",
        "y_hat = np.array([[0.8, 0.1, 0.1],\n",
        "                  [0.2, 0.7, 0.1],\n",
        "                  [0.3, 0.4, 0.3]])\n",
        "y = np.array([0, 1, 2])\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = calculate_confusion_matrix(y_hat, y)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ogmepupunue",
        "outputId": "3e6de631-3054-49a7-dd99-fc478c19fd4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming that the actual output and the desired output of a neural network are given.\n",
        "Complete the code for the following function to calculate the overall cross entropy loss for a batch of data.y_hat is a matrix presenting the actual output. Each row of this matrix is the actual output of the neural network for an input sample. The number of rows in this matrix is equal to the number of samples in the input batch. y is the desired output. Each row is a one-hot representation of the desired class. This means that all entries in each\n",
        "row are zeros except one of them which is equal to 1 indicating the correct class.\n",
        "\n",
        "Notes:\n",
        "Do NOT use TensorFlow or Keras.\n",
        "You may use the numpy helper functions np.nonzero() and np.log() and np.sum()\n",
        "def calculate_overall_cross_entropy_loss(y_hat,y):\n",
        "```\n",
        "Import numpy as np\n",
        "\"\"\" This function calculates the overall cross entropy loss.\n",
        "layer numbers start from zero.\n",
        ":param y_hat: actual output [number_of_samples,number_of_classes].\n",
        ":param Y: Desired output [number_of_samples,number_of_classes].\n",
        ":return: overall cross-entropy loss\n",
        "\"\"\"\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/CSE-5368%20Exercise%20problems%2007.pdf"
      ],
      "metadata": {
        "id": "qWEWW3czvXng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_overall_cross_entropy_loss(y_hat, y):\n",
        "\n",
        "    # Calculate cross entropy loss\n",
        "    cross_entropy_loss = -np.sum(y * np.log(y_hat + 1e-15)) / len(y)\n",
        "\n",
        "    return cross_entropy_loss\n",
        "\n",
        "# Example usage:\n",
        "# Assume y_hat and y are given\n",
        "y_hat = np.array([[0.9, 0.1], [0.2, 0.8], [0.6, 0.4]])\n",
        "y = np.array([[1, 0], [0, 1], [1, 0]])\n",
        "\n",
        "# Calculate overall cross-entropy loss\n",
        "loss = calculate_overall_cross_entropy_loss(y_hat, y)\n",
        "print(\"Overall Cross-Entropy Loss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiSjw1J0vN25",
        "outputId": "2308bc4f-2cc7-483d-d079-f7b4efc0d5d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Cross-Entropy Loss: 0.2797765635793409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the expression: f(x,y) = x^2 + y\n",
        "\n",
        "Given the inputs x=2, y=5, write a Python program, using Tensorflow to\n",
        "print the value of the output f(x,y) and partial derivatives of the f(x,y) with respect to x and y\n",
        "```\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/CSE-5368%20Exercise%20problems%2007.pdf"
      ],
      "metadata": {
        "id": "7nV6YEG8vz-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tZ_6eVLOvXD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the inputs\n",
        "x_value = 2\n",
        "y_value = 5\n",
        "\n",
        "# Define the variables as TensorFlow constants\n",
        "x = tf.constant(x_value, dtype=tf.float32)\n",
        "y = tf.constant(y_value, dtype=tf.float32)\n",
        "\n",
        "# Define the function\n",
        "def f(x, y):\n",
        "    return x**2 + y\n",
        "\n",
        "# Calculate the value of f(x, y)\n",
        "output = f(x, y)\n",
        "\n",
        "# Calculate the partial derivatives of f(x, y) with respect to x and y\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch([x, y])\n",
        "    output = f(x, y)\n",
        "\n",
        "grad_x, grad_y = tape.gradient(output, [x, y])\n",
        "\n",
        "# Print the results\n",
        "print(\"Value of f(x, y):\", output.numpy())\n",
        "print(\"Partial derivative of f(x, y) with respect to x:\", grad_x.numpy())\n",
        "print(\"Partial derivative of f(x, y) with respect to y:\", grad_y.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2eWzouSwHFf",
        "outputId": "51542fef-28df-4d80-fb85-70ea6e6bfb83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value of f(x, y): 9.0\n",
            "Partial derivative of f(x, y) with respect to x: 4.0\n",
            "Partial derivative of f(x, y) with respect to y: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write a program using Tensorflow to implement a neural network with one hidden layer and one output layer.\n",
        "\n",
        "The dimension of the input data data : 3\n",
        "\n",
        "Number of nodes in hidden layer: 10, activation: sigmoid\n",
        "\n",
        "Number of nodes in the output layer: 5 activation : linear\n",
        "\n",
        "Just implement the forward path to calculate and display output. No training loop , or calculation of gradients.\n",
        "\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/CSE-5368%20Exercise%20problems%2007.pdf"
      ],
      "metadata": {
        "id": "un9jbTbVwmIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the input data\n",
        "input_data = tf.constant([[1.0, 2.0, 3.0]])\n",
        "\n",
        "# Define the weights and biases for the hidden layer\n",
        "hidden_layer_weights = tf.Variable(tf.random.normal([3, 10]))\n",
        "hidden_layer_bias = tf.Variable(tf.random.normal([1, 10]))\n",
        "\n",
        "# Calculate the output of the hidden layer\n",
        "hidden_layer_output = tf.sigmoid(tf.matmul(input_data, hidden_layer_weights) + hidden_layer_bias)\n",
        "\n",
        "# Define the weights and biases for the output layer\n",
        "output_layer_weights = tf.Variable(tf.random.normal([10, 5]))\n",
        "output_layer_bias = tf.Variable(tf.random.normal([1, 5]))\n",
        "\n",
        "# Calculate the output of the output layer\n",
        "output = tf.matmul(hidden_layer_output, output_layer_weights) + output_layer_bias\n",
        "\n",
        "# Print the output\n",
        "print(\"Output of the neural network:\")\n",
        "print(output.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqdUjPePwebF",
        "outputId": "44614c3c-2036-464d-8fe2-3425ab3b5fc4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the neural network:\n",
            "[[ 0.8143492 -0.2766054 -2.2184353 -2.1525347 -1.2441318]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fall 2021 Exam 1\n",
        "\n",
        "Q4. Given a single-layer linear associator neural network. Complete the code for the\n",
        "following function (this is similar to the function in the assignment 02).\n",
        " Only implement the delta learning 𝑾𝒏𝒆𝒘 = 𝑾𝒐𝒍𝒅 ൅ 𝜶ሺ𝒕െ𝒂ሻ𝒑𝑻\n",
        " Ignore bias\n",
        "```\n",
        "import numpy as np\n",
        "def train(X, Y, W, batch_size, num_epochs, alpha):\n",
        "\"\"\" Given a set of data points, and the hyperparameters, this function adjusts\n",
        "the weights using the delta learning rule.\n",
        ":param X: Array of input [input_dimensions,n_samples]\n",
        ":param y: Array of desired (target) outputs [number_of_nodes,n_samples].\n",
        ":param W: Array of weights (number_of_nodes,input_dimensiona)\n",
        ":param num_epochs: No. of times training should be repeated over all data\n",
        ":param batch_size: number of samples in a batch\n",
        ":param alpha: Learning rate\n",
        ":return Adjusted weights:\"\"\"\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2021_10_14%20Exam1.pdf"
      ],
      "metadata": {
        "id": "Bmed-uVjxQ6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fall 2021 Exam 1 Q4\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def train(X, Y, W, batch_size, num_epochs, alpha):\n",
        "\n",
        "    n_samples = X.shape[1]\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            X_batch = X[:, i:i+batch_size]\n",
        "            Y_batch = Y[:, i:i+batch_size]\n",
        "            delta_W = alpha * (np.dot(Y_batch - np.dot(W, X_batch), X_batch.T))\n",
        "            W += delta_W\n",
        "    return W\n"
      ],
      "metadata": {
        "id": "fRUnyVTZxQnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Complete the following code using Tensorflow. The code should initialize the weights and biases and compute the output, loss, and gradients, and perform a single weight update of a neural network. Assume the dimension of each input sample is 400.\n",
        "There are 100 nodes with sigmoid activation in the first layer, and 10 output nodes with linear activation. Loss is MSE. Assume you have a batch of inputs called X that has dimensions [20, 400], and a batch of targets called y that has dimensions [20, 10]\n",
        "```\n",
        "import tensorflow as tf     \n",
        "import numpy as np\n",
        "# Create random weights and biases\n",
        "W_1 = tf.Variable(np.random.randn(                 ), trainable=True)\n",
        "b_1 = tf.Variable(np.random.randn(                 ), trainable=True)\n",
        "W_2 = tf.Variable(np.random.randn(                ), trainable=True)\n",
        "b_2 = tf.Variable(np.random.randn(                 ), trainable=True)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "# Calculate output\n",
        "output=\n",
        "# Calculate loss\n",
        "Loss=\n",
        "# Calculate gradients\n",
        "     \n",
        "dW_1, db_1 =\n",
        "     \n",
        "dW_2, db_2 =\n",
        "# Calculate new values of weights and biases\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2021_10_14%20Exam1.pdf"
      ],
      "metadata": {
        "id": "E452L4Whxp6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Dimensionality of input and output\n",
        "input_dim = 400\n",
        "output_dim = 10\n",
        "\n",
        "# Create random weights and biases\n",
        "W_1 = tf.Variable(np.random.randn(input_dim, 100).astype(np.float32), trainable=True)\n",
        "b_1 = tf.Variable(np.random.randn(100).astype(np.float32), trainable=True)\n",
        "W_2 = tf.Variable(np.random.randn(100, output_dim).astype(np.float32), trainable=True)\n",
        "b_2 = tf.Variable(np.random.randn(output_dim).astype(np.float32), trainable=True)\n",
        "\n",
        "# Assume X and y are given\n",
        "X = np.random.randn(20, input_dim).astype(np.float32)\n",
        "y = np.random.randn(20, output_dim).astype(np.float32)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    # Calculate output\n",
        "    hidden_output = tf.sigmoid(tf.matmul(X, W_1) + b_1)\n",
        "    output = tf.matmul(hidden_output, W_2) + b_2\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = tf.reduce_mean(tf.square(output - y))\n",
        "\n",
        "# Calculate gradients\n",
        "grads = tape.gradient(loss, [W_1, b_1, W_2, b_2])\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Update weights and biases using gradient descent\n",
        "W_1.assign_sub(learning_rate * grads[0])\n",
        "b_1.assign_sub(learning_rate * grads[1])\n",
        "W_2.assign_sub(learning_rate * grads[2])\n",
        "b_2.assign_sub(learning_rate * grads[3])\n",
        "\n",
        "# Print the updated values\n",
        "print(\"Updated weights W_1:\\n\", W_1.numpy())\n",
        "print(\"Updated biases b_1:\\n\", b_1.numpy())\n",
        "print(\"Updated weights W_2:\\n\", W_2.numpy())\n",
        "print(\"Updated biases b_2:\\n\", b_2.numpy())\n"
      ],
      "metadata": {
        "id": "piPHEYrawzF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fall 2020 Exam 1 (similar)\n",
        "\n",
        "Complete the code for the following function (this is similar to function in\n",
        "assignment 01). Note that YA and YT are given\n",
        "```\n",
        "import numpy as np\n",
        "def calculate_percent_error(YA,YT):\n",
        "\"\"\" Given a batch of input, actual outputs, and desired outputs, this function\n",
        "calculates percent error. For each sample, if the actual output vector is not\n",
        "exactly the same as the desired output, it is considered one error. Percent\n",
        "error is 100*(number_of_errors/ number_of_samples)\n",
        ":param YA: Array of actual outputs [number_of_nodes, ,n_samples]. Assume that\n",
        "each element of YA is either 0 or 1 (Result of hard-limit activation function).\n",
        "param YT: Array of desired (target) outputs [number_of_nodes ,n_samples].\n",
        "Assume that each element of YT is either 0 or 1\n",
        ":return percent_error\"\"\"\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2022_10_13%20Exam1.pdf"
      ],
      "metadata": {
        "id": "3_T7_1mGyUdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_percent_error(YA, YT):\n",
        "\n",
        "    num_errors = np.sum(np.any(YA != YT, axis=0))\n",
        "    percent_error = 100 * num_errors / YA.shape[1]\n",
        "    return percent_error\n",
        "\n",
        "# Example usage:\n",
        "# Assuming YA and YT are given\n",
        "YA = np.array([[0, 1, 1, 0], [1, 0, 1, 0]])\n",
        "YT = np.array([[0, 1, 0, 0], [1, 0, 1, 1]])\n",
        "\n",
        "# Calculate percent error\n",
        "error = calculate_percent_error(YA, YT)\n",
        "print(\"Percent Error:\", error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8fJunIAydkH",
        "outputId": "8348d36f-7dc8-48ce-fa17-17d21ddbe97e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percent Error: 50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fall 2022 Quiz 5\n",
        "\n",
        "Complete the following function.\n",
        "Only use numpy.\n",
        "DO NOT USE Tensorflow, Keras, or any other packages.\n",
        "```\n",
        "import numpy as np\n",
        "def calculate_confusion_matrix(y_hat, y):\n",
        "\"\"\" Given the actual outputs and the desired output index of a neural\n",
        "network, this function calculates the confusion matrix.\n",
        ":param y_hat: Array of actual outputs [num_of samples,num_of classes]\n",
        ":param y: Array of desired (target) outputs [num_of samples]. This\n",
        "array includes the indexes of the desired (true) class.\n",
        ":return confusion_matrix[number_of_classes,number_of_classes].\n",
        "Confusion matrix should be shown as the number of times that an input\n",
        "of class n is classified as class m.\"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "uE_Ja5IdymV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_confusion_matrix(y_hat, y):\n",
        "\n",
        "    num_classes = y.shape[0]\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
        "    for i in range(y_hat.shape[0]):\n",
        "      confusion_matrix[i][np.argmax(y_hat[i])]  += 1\n",
        "    return confusion_matrix\n"
      ],
      "metadata": {
        "id": "Fba5xOPxyfe8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input data\n",
        "y_hat = np.array([[0.8, 0.1, 0.1],\n",
        "                  [0.2, 0.7, 0.1],\n",
        "                  [0.3, 0.4, 0.3]])\n",
        "y = np.array([0, 1, 2])\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = calculate_confusion_matrix(y_hat, y)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8PkSGXUy7fQ",
        "outputId": "2804ddad-43f2-4a61-872f-0804de1adc54"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z29kopq0y0Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fall 2022 Exam 02 (similar)\n",
        "\n",
        "Complete the following function using tensorflow (do not use keras) to create and train a two-layer neural network. The first layer should have 100 ReLU nodes.\n",
        "The output layer should have linear nodes.\n",
        "Your loss function should be mean squared error.\n",
        "Anything not specified in this description should be inferred from the\n",
        "function’s parameters and not hardcoded.\n",
        "Code should include initializing weights, training loop with forward pass, gradient calculation, and weight updates.\n",
        "You may assume the entire dataset is one batch and you do not need to split the data into batches.\n",
        "```\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "def create_and_train_nn(X, Y, num_epochs, alpha):\n",
        "\"\"\"  \n",
        ":param X: Array of input [n_samples,input_dimensions]\n",
        ":param y: Array of desired (target) outputs [n_samples , target_dimension].\n",
        ":param num_epochs: No. of times training should be repeated over all data\n",
        ":param alpha: Learning rate:\"\"\"\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2022_12_01%20Exam2.pdf"
      ],
      "metadata": {
        "id": "PX6DxM-ZzJzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_and_train_nn(X, Y, num_epochs, alpha):\n",
        "    \"\"\"\n",
        "    Create and train a two-layer neural network using TensorFlow.\n",
        "\n",
        "    :param X: Array of input [n_samples, input_dimensions]\n",
        "    :param Y: Array of desired (target) outputs [n_samples, target_dimension]\n",
        "    :param num_epochs: No. of times training should be repeated over all data\n",
        "    :param alpha: Learning rate\n",
        "    :return: trained weights and biases\n",
        "    \"\"\"\n",
        "    # Get input and output dimensions\n",
        "    input_dim = X.shape[1]\n",
        "    output_dim = Y.shape[1]\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    W1 = tf.Variable(tf.random.normal([input_dim, 100]), trainable=True)\n",
        "    b1 = tf.Variable(tf.zeros([100]), trainable=True)\n",
        "    W2 = tf.Variable(tf.random.normal([100, output_dim]), trainable=True)\n",
        "    b2 = tf.Variable(tf.zeros([output_dim]), trainable=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Hidden layer\n",
        "            hidden_output = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "            # Output layer\n",
        "            output = tf.matmul(hidden_output, W2) + b2\n",
        "            # Calculate mean squared error\n",
        "            loss = tf.reduce_mean(tf.square(output - Y))\n",
        "\n",
        "        # Calculate gradients\n",
        "        gradients = tape.gradient(loss, [W1, b1, W2, b2])\n",
        "\n",
        "        # Update weights and biases\n",
        "        W1.assign_sub(alpha * gradients[0])\n",
        "        b1.assign_sub(alpha * gradients[1])\n",
        "        W2.assign_sub(alpha * gradients[2])\n",
        "        b2.assign_sub(alpha * gradients[3])\n",
        "\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss.numpy()}\")\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X and Y are given\n",
        "X = np.random.randn(100, 10)\n",
        "Y = np.random.randn(100, 5)\n",
        "num_epochs = 1000\n",
        "alpha = 0.01\n",
        "\n",
        "# Train the neural network\n",
        "W1, b1, W2, b2 = create_and_train_nn(X, Y, num_epochs, alpha)\n"
      ],
      "metadata": {
        "id": "x56ac9l7zWDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spring 2023 Quiz 06\n",
        "Complete the code for the following function.\n",
        "This function is the same as the initialization part of the Assignment_01.\n",
        "```\n",
        "import numpy as np\n",
        "def multi_layer_nn(X_train,Y_train,layers,seed=2):\n",
        "# This function Initializes all the weights and\n",
        "# returns weight matrices in a list\n",
        "# bias should be included in the weight matrix.\n",
        "# X_train: Array of input for training [input_dimensions,nof_train_samples]\n",
        "# Y_train: Array of desired outputs [output_dimensions,nof_train_samples]\n",
        "# layers: array of integers representing number of nodes in each layer\n",
        "# return: This function should return a list of weight matrices.\n",
        "# Each element of the list corresponds to the weight matrix of the\n",
        "# corresponding layer.|\n",
        "# Initialize the weights for each layer by:\n",
        "# np.random.seed(seed)\n",
        "# np.random.randn()\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2023_03_09%20Quiz%2006.pdf"
      ],
      "metadata": {
        "id": "ILDdE01Dzj04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def multi_layer_nn(X_train, Y_train, layers, seed=2):\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    num_layers = len(layers)\n",
        "    weights = []\n",
        "\n",
        "    # Initialize weights for each layer\n",
        "    for i in range(num_layers - 1):\n",
        "        input_dim = layers[i]\n",
        "        output_dim = layers[i + 1]\n",
        "        weight_matrix = np.random.randn(output_dim, input_dim + 1)  # Add 1 for bias\n",
        "        weights.append(weight_matrix)\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "gxxJGoPizpSe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "X_train = np.random.randn(10, 100)  # Example input data\n",
        "Y_train = np.random.randn(5, 100)   # Example output data\n",
        "layers = [10, 20, 5]  # Example layer configuration\n",
        "\n",
        "# Initialize weights\n",
        "weights = multi_layer_nn(X_train, Y_train, layers)\n",
        "print(\"Initialized weights:\")\n",
        "for i, weight_matrix in enumerate(weights):\n",
        "    print(f\"Layer {i+1}: {weight_matrix.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rT2V63uztu3",
        "outputId": "3c6138e5-b0f6-4e70-fd00-68a898dcf10b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized weights:\n",
            "Layer 1: (20, 11)\n",
            "Layer 2: (5, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spring 2023 Quiz 07\n",
        "\n",
        "Complete the code for the following function.\n",
        "USE numpy only\n",
        "DO NOT USE tensorflow or keras\n",
        "```\n",
        "import numpy as np\n",
        "def calculate_svm (yhat,yt):\n",
        "# This function calculates the SVM error for the entire data set\n",
        "# yhat: Array of actual outputs [num_of_samples,num_of_classes]\n",
        "# yt: Array of desired outputs [num_of_samples]\n",
        "# Each element of yt array is the index of the true class.,\n",
        "# return: SVM for the entire data set. A single float number.\n",
        "# Return value is the average of all the SVMs for the samples.\n",
        "# Use the following equation for SVM loss for one sample.\n",
        "# Assume delta is equal to 1\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2023_03_30%20Quiz%2007.pdf"
      ],
      "metadata": {
        "id": "a_NS45oxz6ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_svm_v2_my(yhat, yt):\n",
        "    delta = 1\n",
        "    total_loss = 0\n",
        "    num_samples = yt.shape[0]\n",
        "    for sample in range(num_samples):\n",
        "\n",
        "        margin = delta + yhat[sample] - yhat[sample][yt[sample]]\n",
        "        print(yhat[sample][yt[sample]])\n",
        "        print(yhat[sample])\n",
        "        print(margin)\n",
        "        loss = np.maximum(0.0, margin)\n",
        "        loss[yt[sample]] = 0\n",
        "\n",
        "        total_loss += np.sum(loss)\n",
        "    return total_loss / num_samples"
      ],
      "metadata": {
        "id": "PpvJRxdVzuMl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "yhat = np.array([[0.2, 0.3, 0.5],\n",
        "                 [0.1, 0.5, 0.4],\n",
        "                 [0.6, 0.2, 0.2]])\n",
        "yt = np.array([0, 1, 2])  # Sample true labels\n",
        "\n",
        "# Calculate SVM error using the provided function\n",
        "svm_error = calculate_svm_v2_my(yhat, yt)\n",
        "\n",
        "# Print the result\n",
        "print(\"Third implementation output:\", svm_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEGs6zJ30OXC",
        "outputId": "b8c320c8-ac33-4c41-9a7c-02ca580c9a89"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2\n",
            "[0.2 0.3 0.5]\n",
            "[1.  1.1 1.3]\n",
            "0.5\n",
            "[0.1 0.5 0.4]\n",
            "[0.6 1.  0.9]\n",
            "0.2\n",
            "[0.6 0.2 0.2]\n",
            "[1.4 1.  1. ]\n",
            "Third implementation output: 2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spring 2023 Quiz 11\n",
        "Using tensorflow, complete the following function to create and train a two-layer neural\n",
        "network. The first layer has 12 sigmoid nodes.\n",
        "The output layer has linear nodes.\n",
        "Loss function should be MSE. Anything not specified in the description should be inferred\n",
        "from the function’s parameters and not hardcoded.\n",
        "Code should include initializing weights, training loop with forward pass, gradient\n",
        "calculation, and weight updates.\n",
        "You may assume the entire dataset is one batch.\n",
        "```\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "def create_and_train_nn(X, Y, epochs, alpha):\n",
        "\"\"\"  \n",
        ":param X: Array of input [n_samples,input_dimensions]\n",
        ":param y: Array of desired outputs [n_samples , target_dimension].\n",
        ":param epochs: number of epochs\n",
        ":param alpha: Learning rate:\n",
        ":return w1, w2 Weight matrices.\"\"\"\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2023_04_27%20Quiz%2009-14.pdf"
      ],
      "metadata": {
        "id": "WclEFX9z0YzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_and_train_nn(X, Y, epochs, alpha):\n",
        "\n",
        "    # Get input and output dimensions\n",
        "    input_dim = X.shape[1]\n",
        "    output_dim = Y.shape[1]\n",
        "\n",
        "    # Define the number of nodes in the hidden layer\n",
        "    hidden_nodes = 12\n",
        "\n",
        "    # Initialize weights randomly\n",
        "    w1 = tf.Variable(tf.random.normal([input_dim, hidden_nodes]), trainable=True)\n",
        "    w2 = tf.Variable(tf.random.normal([hidden_nodes, output_dim]), trainable=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Hidden layer\n",
        "            hidden_output = tf.sigmoid(tf.matmul(X, w1))\n",
        "            # Output layer\n",
        "            output = tf.matmul(hidden_output, w2)\n",
        "            # Calculate mean squared error\n",
        "            loss = tf.reduce_mean(tf.square(output - Y))\n",
        "\n",
        "        # Calculate gradients\n",
        "        gradients = tape.gradient(loss, [w1, w2])\n",
        "\n",
        "        # Update weights using gradient descent\n",
        "        w1.assign_sub(alpha * gradients[0])\n",
        "        w2.assign_sub(alpha * gradients[1])\n",
        "\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss.numpy()}\")\n",
        "\n",
        "    return w1, w2\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X and Y are given\n",
        "X = np.random.randn(100, 10)  # Example input data\n",
        "Y = np.random.randn(100, 5)   # Example output data\n",
        "epochs = 1000\n",
        "alpha = 0.01\n",
        "\n",
        "# Train the neural network\n",
        "w1, w2 = create_and_train_nn(X, Y, epochs, alpha)\n"
      ],
      "metadata": {
        "id": "AufWPXyF0OnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#fall 2023 Exam 1\n",
        "2. Complete the code for the following function.\n",
        "This function is the same as the initialization part of the Assignment_01.\n",
        "```\n",
        "import numpy as np\n",
        "def multi_layer_nn(X_train,Y_train,layers):\n",
        "# This function Initializes all the weights and returns weight\n",
        "# matrices in a list\n",
        "# bias should be included in the weight matrix.\n",
        "# X_train: Array of input for training [input_dimension,nof_train_samples]\n",
        "# Y_train: Array of desired outputs [output_dimension,nof_train_samples]\n",
        "# layers: array of integers representing number of nodes in each layer\n",
        "# return: This function should return a list of weight matrices.\n",
        "# Each element of the list should be a numpy array corresponds to the\n",
        "# weight matrix of the corresponding layer.\n",
        "# Initialize all the weights to zero\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2023_10_12%20Exam%2001.pdf"
      ],
      "metadata": {
        "id": "434c3Wii0_mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def multi_layer_nn(X_train, Y_train, layers):\n",
        "\n",
        "    num_layers = len(layers)\n",
        "    weights = []\n",
        "\n",
        "    for i in range(num_layers - 1):\n",
        "        input_dim = layers[i]\n",
        "        output_dim = layers[i + 1]\n",
        "        weight_matrix = np.zeros((output_dim, input_dim + 1))  # Add 1 for bias\n",
        "        weights.append(weight_matrix)\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "iESpL80M026X"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage:\n",
        "X_train = np.random.randn(10, 100)  # Example input data\n",
        "Y_train = np.random.randn(5, 100)   # Example output data\n",
        "layers = [10, 20, 5]  # Example layer configuration\n",
        "\n",
        "# Initialize weights\n",
        "weights = multi_layer_nn(X_train, Y_train, layers)\n",
        "print(\"Initialized weights:\")\n",
        "for i, weight_matrix in enumerate(weights):\n",
        "    print(f\"Layer {i+1}: {weight_matrix.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppFpuGC31PkI",
        "outputId": "375c3c64-82a0-4036-fde4-82de13a45310"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized weights:\n",
            "Layer 1: (20, 11)\n",
            "Layer 2: (5, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using tensorflow, complete the following function to create and train a two-layer neural network. The first layer has 7 sigmoid nodes. The output layer has linear\n",
        "nodes. Loss function should be MSE. Anything not specified in the description should be\n",
        "inferred from the function’s arguments and not hardcoded.\n",
        "Code should include initializing weights, training loop with forward pass, gradient calculation,\n",
        "and weight updates.\n",
        "You may assume the entire dataset is one batch.\n",
        "```\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "def create_and_train_nn(X, Y, epochs, alpha):\n",
        "\"\"\"  \n",
        ":param X: Array of input [n_samples,input_dimensions]\n",
        ":param y: Array of desired outputs [n_samples , target_dimension].\n",
        ":param epochs: number of epochs\n",
        ":param alpha: Learning rate:\n",
        ":return w1, w2 (w1 and w2 are the weight matrices after the training is done).\"\"\"\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2023_10_12%20Exam%2001.pdf"
      ],
      "metadata": {
        "id": "4HK47wS_1XLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_and_train_nn(X, Y, epochs, alpha):\n",
        "\n",
        "    # Get input and output dimensions\n",
        "    input_dim = X.shape[1]\n",
        "    output_dim = Y.shape[1]\n",
        "\n",
        "    # Define the number of nodes in the hidden layer\n",
        "    hidden_nodes = 7\n",
        "\n",
        "    # Initialize weights randomly\n",
        "    w1 = tf.Variable(tf.random.normal([input_dim, hidden_nodes]), trainable=True)\n",
        "    w2 = tf.Variable(tf.random.normal([hidden_nodes, output_dim]), trainable=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Hidden layer\n",
        "            hidden_output = tf.nn.sigmoid(tf.matmul(X, w1))\n",
        "            # Output layer\n",
        "            output = tf.matmul(hidden_output, w2)\n",
        "            # Calculate mean squared error\n",
        "            loss = tf.reduce_mean(tf.square(output - Y))\n",
        "\n",
        "        # Calculate gradients\n",
        "        gradients = tape.gradient(loss, [w1, w2])\n",
        "\n",
        "        # Update weights using gradient descent\n",
        "        w1.assign_sub(alpha * gradients[0])\n",
        "        w2.assign_sub(alpha * gradients[1])\n",
        "\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss.numpy()}\")\n",
        "\n",
        "    return w1, w2\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X and Y are given\n",
        "X = np.random.randn(100, 10)  # Example input data\n",
        "X = tf.convert_to_tensor(X, dtype=tf.float64)\n",
        "Y = np.random.randn(100, 5)   # Example output data\n",
        "epochs = 1000\n",
        "alpha = 0.01\n",
        "\n",
        "# Train the neural network\n",
        "w1, w2 = create_and_train_nn(X, Y, epochs, alpha)\n"
      ],
      "metadata": {
        "id": "FwfeNO4u1vJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fall 2023 Quiz 05\n",
        "\n",
        "Do NOT use keras or Tensorflow.\n",
        "Only use numpy.\n",
        "```\n",
        "import numpy as np\n",
        "def calculate_softmax(y):\n",
        "# y is a numpy array [number of samples, number_of_classes]\n",
        "# Return softmax as numpy array.\n",
        "[number_of_samples,number_of_classes]\n",
        "```\n",
        "https://ranger.uta.edu/~kamangar/CSE_5368_SP24/LinkedDocuments/2023_11_07%20Quiz%2005%20and%2006.pdf"
      ],
      "metadata": {
        "id": "kZd7ypbx1uzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_softmax(y):\n",
        "    # y is a numpy array [number of samples, number_of_classes]\n",
        "    # Return softmax as numpy array.\n",
        "    softmax_output = np.zeros((y.shape[0], y.shape[1]))\n",
        "    for i in range(y.shape[0]):\n",
        "      den = np.sum(np.exp(y[i]))\n",
        "      for j in range(y.shape[1]):\n",
        "        softmax_output[i][j] = np.exp(y[i][j])/den\n",
        "        #print(softmax_output[i][j])\n",
        "        #print(den)\n",
        "    return softmax_output\n",
        "\n",
        "# Example input data\n",
        "input_data = np.array([[1, 2, 3], [4, 5.4, 6], [7, 8, 9.1]])\n",
        "\n",
        "'''\n",
        "1  2  3\n",
        "4  5.4  6\n",
        "7  8  9.1\n",
        "'''\n",
        "\n",
        "# Calculate softmax\n",
        "softmax_output = calculate_softmax(input_data)\n",
        "\n",
        "# Print output\n",
        "print(\"Softmax Output:\")\n",
        "print(softmax_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0KI9l2R2FwF",
        "outputId": "fcf18ecb-0e45-4b9b-afba-bf4933e4fe88"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Output:\n",
            "[[0.09003057 0.24472847 0.66524096]\n",
            " [0.08035836 0.32586922 0.59377242]\n",
            " [0.08414355 0.22872589 0.68713055]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lx5Cb4om1WVU"
      }
    }
  ]
}